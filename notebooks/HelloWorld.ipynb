{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98a26a-1325-4dbf-870a-b486e6c544ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark_utils import quick_start\n",
    "spark = quick_start(\"TestConnection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba9c5d-307a-40bf-8878-a1717b2c0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic S3 connectivity\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS test_db LOCATION 's3a://delta-lake/test_db/'\")\n",
    "\n",
    "# Test regular Parquet (should work)\n",
    "data = [(1, \"Alice\", 25), (2, \"Bob\", 30)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\"])\n",
    "df.write.mode(\"overwrite\").parquet(\"s3a://delta-lake/test_parquet\")\n",
    "\n",
    "# Read it back\n",
    "test_df = spark.read.parquet(\"s3a://delta-lake/test_parquet\")\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b892d0b6-1d79-4112-a7fe-1846ee1a3f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just read the existing Delta table directly\n",
    "df = spark.read.format(\"delta\").load(\"s3a://delta-lake/bronze/github/keyword_extractions\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54dbf0-0907-4aee-9f4b-72ea0c89584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working executor info for Spark 4.0\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"=== SPARK CLUSTER INFO ===\")\n",
    "print(f\"Application ID: {sc.applicationId}\")\n",
    "print(f\"Application Name: {sc.appName}\")\n",
    "print(f\"Spark Version: {sc.version}\")\n",
    "print(f\"Default parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"Master: {sc.master}\")\n",
    "\n",
    "# Test actual parallel execution\n",
    "print(\"\\n=== TESTING PARALLEL EXECUTION ===\")\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "# Create work that will be distributed across executors\n",
    "test_data = sc.parallelize(range(1000), 8)\n",
    "result = test_data.map(lambda x: x * x).filter(lambda x: x % 2 == 0).count()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Processed 1000 numbers across {test_data.getNumPartitions()} partitions\")\n",
    "print(f\"Result: {result} even squares\")\n",
    "print(f\"Processing time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Check what we can access\n",
    "print(\"\\n=== AVAILABLE STATUS TRACKER METHODS ===\")\n",
    "status_tracker = sc.statusTracker()\n",
    "available_methods = [method for method in dir(status_tracker) if not method.startswith('_')]\n",
    "print(f\"Available methods: {available_methods}\")\n",
    "\n",
    "print(f\"\\nüåê For detailed executor info, open: http://localhost:4041\")\n",
    "print(\"   Go to 'Executors' tab to see:\")\n",
    "print(\"   - Number of executors\")\n",
    "print(\"   - Cores per executor\") \n",
    "print(\"   - Memory usage\")\n",
    "print(\"   - Task distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7911e1c8-5fce-41b3-8740-00543f5a8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter_core\n",
    "import jupyterlab\n",
    "print(f\"Jupyter Core: {jupyter_core.__version__}\")\n",
    "print(f\"JupyterLab: {jupyterlab.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ca6bcf-823a-405f-bd5d-727480ed46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the correct method names for Spark 4.0\n",
    "status_tracker = sc.statusTracker()\n",
    "\n",
    "print(\"=== CURRENT ACTIVITY ===\")\n",
    "try:\n",
    "    active_jobs = status_tracker.getActiveJobsIds()\n",
    "    active_stages = status_tracker.getActiveStageIds()\n",
    "    print(f\"Active jobs: {len(active_jobs)}\")\n",
    "    print(f\"Active stages: {len(active_stages)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Status check: {e}\")\n",
    "\n",
    "# Check your Delta Lake data processing\n",
    "print(\"\\n=== DELTA LAKE PROCESSING TEST ===\")\n",
    "df = spark.read.format(\"delta\").load(\"s3a://delta-lake/bronze/github/keyword_extractions\")\n",
    "\n",
    "start_time = time.time()\n",
    "count = df.count()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Delta table rows: {count}\")\n",
    "print(f\"Read time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Partitions used: {df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a21955-80b7-4410-bffe-14fe8b86354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the full picture of your worker utilization\n",
    "print(\"=== CLUSTER RESOURCE SUMMARY ===\")\n",
    "print(\"Master: spark://spark-master:7077\")\n",
    "print(f\"Available cores: {sc.defaultParallelism}\")\n",
    "print(f\"Partition strategy: {df.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# Test with different partition counts\n",
    "print(\"\\n=== PARTITION OPTIMIZATION TEST ===\")\n",
    "df = spark.read.format(\"delta\").load(\"s3a://delta-lake/bronze/github/keyword_extractions\")\n",
    "\n",
    "# Try repartitioning to use more parallelism\n",
    "df_repartitioned = df.repartition(4)\n",
    "print(f\"Repartitioned to: {df_repartitioned.rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "start_time = time.time()\n",
    "sample_data = df_repartitioned.sample(0.1).collect()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Sample processing time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Sample size: {len(sample_data)} rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
