#!/bin/bash
# =============================================================================
# AIRFLOW 3.0 BATCH-AWARE STREAMING MANAGER
# =============================================================================
# 
# CRITICAL AIRFLOW 3.0 NOTES (to prevent future confusion):
# 
# 1. API ENDPOINTS - Airflow 3.0 uses FastAPI-based REST API v2:
#    ✅ CORRECT: /api/v2/dags/{dag_id}/dag-runs (hyphen, not camelCase)
#    ❌ WRONG:   /api/v1/dags/{dag_id}/dagRuns (old v1, deprecated)
#    ❌ WRONG:   /api/experimental/* (completely removed)
#
# 2. CLI COMMANDS - Airflow 3.0 requires automation flags:
#    ✅ CORRECT: airflow dags pause dag_id -y (with -y for scripts)
#    ❌ WRONG:   airflow dags pause dag_id (will prompt interactively)
#
# 3. AUTHENTICATION - Basic auth required for API calls:
#    ✅ CORRECT: curl -u "$AIRFLOW_USER:$AIRFLOW_PASS" (from env vars)
#    ❌ WRONG:   Hardcoded credentials in script
#
# 4. DAG REQUIREMENTS - For external trigger control:
#    ✅ REQUIRED: schedule=None in DAG definition
#    ❌ WRONG:    schedule_interval='@daily' (causes scheduling conflicts)
#
# 5. SAFETY PATTERNS - Prevent queue overflow:
#    ✅ REQUIRED: Check running/queued state before trigger
#    ✅ REQUIRED: Implement trigger cooldowns (300s minimum)
#    ✅ REQUIRED: Validate DAG exists before operations
#
# =============================================================================

set -euo pipefail

# Load environment variables from .env if it exists
if [ -f "$(dirname "$0")/../.env" ]; then
    export $(grep -v '^#' "$(dirname "$0")/../.env" | xargs)
fi

# =============================================================================
# CONFIGURATION
# =============================================================================

# Batch Job Schedule (24-hour format)
BATCH_START_HOUR=13
BATCH_END_HOUR=17
PREP_MINUTES=30
RECOVERY_MINUTES=30

# DAG Lists - REQUIREMENT: All streaming DAGs MUST have schedule=None
STREAMING_DAGS=(
    "producer_github_events"
    "consumer_github_events"
)

BATCH_DAGS=(
    "daily_github_bronze"
    "daily_github_processing_analytics"
    "daily_jobs_bronze"
)

# System Thresholds
DISK_CLEANUP_AT=60
DISK_EMERGENCY_AT=75
BATCH_DISK_CLEANUP=50
BATCH_DISK_EMERGENCY=70

# Monitoring Intervals (seconds)
STREAMING_CHECK=300
BATCH_CHECK=1800
TRANSITION_CHECK=60

# Safety Settings
TRIGGER_COOLDOWN=300
MAX_QUEUED_RUNS=2
SAFE_MODE=true

# =============================================================================
# SYSTEM SETUP
# =============================================================================

LOG_FILE="/tmp/batch_streaming_$(date +%Y%m%d).log"
TRIGGER_FILE="/tmp/dag_triggers"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m'

# Logging functions
log() { echo -e "${GREEN}[$(date '+%H:%M:%S')]${NC} $1" | tee -a "$LOG_FILE"; }
warn() { echo -e "${YELLOW}[$(date '+%H:%M:%S')] ⚠️${NC} $1" | tee -a "$LOG_FILE"; }
error() { echo -e "${RED}[$(date '+%H:%M:%S')] ❌${NC} $1" | tee -a "$LOG_FILE"; }
info() { echo -e "${BLUE}[$(date '+%H:%M:%S')] ℹ️${NC} $1" | tee -a "$LOG_FILE"; }
phase() { echo -e "${PURPLE}[$(date '+%H:%M:%S')] 🔄${NC} $1" | tee -a "$LOG_FILE"; }

# =============================================================================
# AUTHENTICATION (SECURE)
# =============================================================================

# SECURITY: Never hardcode credentials - use environment variables
get_auth_header() {
    # Check for standard naming first, then fall back to .env naming
    local user="${AIRFLOW_USER:-${_AIRFLOW_WWW_USER_USERNAME:-}}"
    local pass="${AIRFLOW_PASS:-${_AIRFLOW_WWW_USER_PASSWORD:-}}"
    
    if [ -z "$user" ] || [ -z "$pass" ]; then
        error "Authentication environment variables required"
        echo "Set AIRFLOW_USER and AIRFLOW_PASS, or"
        echo "Source .env file: source .env"
        exit 1
    fi
    echo "${user}:${pass}"
}

# =============================================================================
# TIME MANAGEMENT
# =============================================================================

get_current_hour() { date +%H | sed 's/^0*//'; }
get_current_minute() { date +%M | sed 's/^0*//'; }

get_current_time_minutes() {
    local hour=$(get_current_hour)
    local minute=$(get_current_minute)
    echo $((hour * 60 + minute))
}

get_operation_mode() {
    local current_minutes=$(get_current_time_minutes)
    local prep_start=$((BATCH_START_HOUR * 60 - PREP_MINUTES))
    local batch_start=$((BATCH_START_HOUR * 60))
    local batch_end=$((BATCH_END_HOUR * 60))
    local recovery_end=$((batch_end + RECOVERY_MINUTES))
    
    if [ $current_minutes -ge $prep_start ] && [ $current_minutes -lt $batch_start ]; then
        echo "prep"
    elif [ $current_minutes -ge $batch_start ] && [ $current_minutes -lt $batch_end ]; then
        echo "batch"
    elif [ $current_minutes -ge $batch_end ] && [ $current_minutes -lt $recovery_end ]; then
        echo "recovery"
    else
        echo "streaming"
    fi
}

get_minutes_until_next_phase() {
    local current_minutes=$(get_current_time_minutes)
    local mode=$(get_operation_mode)
    
    case $mode in
        prep) echo $((BATCH_START_HOUR * 60 - current_minutes)) ;;
        batch) echo $((BATCH_END_HOUR * 60 - current_minutes)) ;;
        recovery) echo $(((BATCH_END_HOUR * 60 + RECOVERY_MINUTES) - current_minutes)) ;;
        streaming)
            local next_prep=$((BATCH_START_HOUR * 60 - PREP_MINUTES))
            if [ $current_minutes -lt $next_prep ]; then
                echo $((next_prep - current_minutes))
            else
                echo $(((24 * 60 + next_prep) - current_minutes))
            fi ;;
    esac
}

# =============================================================================
# SYSTEM MONITORING
# =============================================================================

get_disk_usage() { df / | awk 'NR==2 {print int($5)}'; }
get_disk_usage_gb() { df -h / | awk 'NR==2 {print $3 "/" $2 " (" $5 ")"}'; }

# AIRFLOW 3.0 API: Use correct v2 endpoint with hyphenated URL
is_dag_running() {
    local dag_id="$1"
    local auth=$(get_auth_header)
    
    # CRITICAL: /api/v2/dags/{dag_id}/dag-runs (note the hyphen!)
    local result=$(docker exec airflow-spark-airflow-apiserver-1 \
        curl -s -u "$auth" "http://localhost:8080/api/v2/dags/${dag_id}/dag-runs?dag_run_state=running&dag_run_state=queued" 2>/dev/null | \
        grep -c '"state":"running"\|"state":"queued"' 2>/dev/null || echo "0")
    [ "$result" -gt 0 ]
}

# AIRFLOW 3.0 CLI: More reliable than API for complex queries
is_dag_running_cli() {
    local dag_id="$1"
    docker exec airflow-spark-airflow-scheduler-1 \
        airflow dags list-runs "$dag_id" 2>/dev/null | \
        awk 'NR>2 {print $3}' | head -5 | \
        grep -q "running\|queued" 2>/dev/null || return 1
}

count_queued_runs() {
    local dag_id="$1"
    docker exec airflow-spark-airflow-scheduler-1 \
        airflow dags list-runs "$dag_id" 2>/dev/null | \
        awk 'NR>2 {print $3}' | \
        grep -c "queued" 2>/dev/null || echo "0"
}

get_streaming_status() {
    local status_parts=()
    
    for dag in "${STREAMING_DAGS[@]}"; do
        local short_name=$(echo "$dag" | sed 's/.*_//')
        if is_dag_running_cli "$dag"; then
            status_parts+=("${short_name}:✅")
        else
            status_parts+=("${short_name}:❌")
        fi
    done
    
    echo "${status_parts[*]}"
}

get_batch_status() {
    local running=0
    local total=${#BATCH_DAGS[@]}
    
    for dag in "${BATCH_DAGS[@]}"; do
        if is_dag_running_cli "$dag"; then
            running=$((running + 1))
        fi
    done
    
    echo "${running}/${total}"
}

# =============================================================================
# TRIGGER TRACKING (SAFETY)
# =============================================================================

get_last_trigger_time() {
    local dag_id="$1"
    if [ -f "$TRIGGER_FILE" ]; then
        grep "^${dag_id}:" "$TRIGGER_FILE" 2>/dev/null | cut -d: -f2 || echo "0"
    else
        echo "0"
    fi
}

set_last_trigger_time() {
    local dag_id="$1"
    local timestamp=$(date +%s)
    
    touch "$TRIGGER_FILE"
    grep -v "^${dag_id}:" "$TRIGGER_FILE" > "${TRIGGER_FILE}.tmp" 2>/dev/null || true
    echo "${dag_id}:${timestamp}" >> "${TRIGGER_FILE}.tmp"
    mv "${TRIGGER_FILE}.tmp" "$TRIGGER_FILE"
}

can_trigger_dag() {
    local dag_id="$1"
    local last_trigger=$(get_last_trigger_time "$dag_id")
    local current_time=$(date +%s)
    local time_diff=$((current_time - last_trigger))
    
    if [ $time_diff -lt $TRIGGER_COOLDOWN ]; then
        warn "DAG $dag_id cooldown active (${time_diff}s < ${TRIGGER_COOLDOWN}s)"
        return 1
    fi
    
    return 0
}

# =============================================================================
# DAG MANAGEMENT (AIRFLOW 3.0 COMPLIANT)
# =============================================================================

dag_exists() {
    local dag_id="$1"
    docker exec airflow-spark-airflow-scheduler-1 \
        airflow dags details "$dag_id" >/dev/null 2>&1
}

# AIRFLOW 3.0: Safe DAG stopping with proper CLI flags
stop_dag_safely() {
    local dag_id="$1"
    info "Stopping DAG: $dag_id"
    
    if ! dag_exists "$dag_id"; then
        warn "DAG $dag_id does not exist"
        return 1
    fi
    
    # CRITICAL: Use -y flag for automation (no interactive prompts)
    docker exec airflow-spark-airflow-scheduler-1 \
        airflow dags pause "$dag_id" -y 2>/dev/null || warn "Could not pause $dag_id"
    
    # Clear only running tasks
    docker exec airflow-spark-airflow-scheduler-1 \
        airflow tasks clear "$dag_id" --yes --only-running 2>/dev/null || warn "Could not clear running tasks"
}

# AIRFLOW 3.0: Safe DAG starting with validation
start_dag_safely() {
    local dag_id="$1"
    
    if ! dag_exists "$dag_id"; then
        error "DAG $dag_id does not exist - check deployment"
        return 1
    fi
    
    # Safety checks
    if [ "$SAFE_MODE" = true ]; then
        local queued=$(count_queued_runs "$dag_id")
        if [ "$queued" -gt $MAX_QUEUED_RUNS ]; then
            warn "DAG $dag_id has $queued queued runs (max: $MAX_QUEUED_RUNS)"
            return 1
        fi
        
        if ! can_trigger_dag "$dag_id"; then
            return 1
        fi
    fi
    
    info "Starting DAG: $dag_id"
    
    # Unpause with automation flag
    docker exec airflow-spark-airflow-scheduler-1 \
        airflow dags unpause "$dag_id" -y 2>/dev/null || warn "Could not unpause $dag_id"
    
    # Only trigger if not already running
    if ! is_dag_running_cli "$dag_id"; then
        docker exec airflow-spark-airflow-scheduler-1 \
            airflow dags trigger "$dag_id" 2>/dev/null && \
            set_last_trigger_time "$dag_id" || \
            warn "Could not trigger $dag_id"
    else
        info "DAG $dag_id already running/queued"
    fi
}

stop_all_streaming_dags() {
    for dag in "${STREAMING_DAGS[@]}"; do
        stop_dag_safely "$dag"
    done
}

start_all_streaming_dags() {
    for dag in "${STREAMING_DAGS[@]}"; do
        start_dag_safely "$dag"
        sleep 30  # Prevent overwhelming scheduler
    done
}

# =============================================================================
# CLEANUP OPERATIONS
# =============================================================================

cleanup_airflow_logs() {
    info "Cleaning Airflow logs"
    for container in scheduler worker triggerer dag-processor apiserver; do
        docker exec "airflow-spark-airflow-${container}-1" bash -c "
            find /opt/airflow/logs -type f -name '*.log' -mmin +120 -delete 2>/dev/null || true
            find /opt/airflow/logs -type d -empty -delete 2>/dev/null || true
        " 2>/dev/null || warn "Could not clean logs in $container"
    done
}

cleanup_spark_resources() {
    info "Cleaning Spark resources"
    for worker in spark-master spark-worker-1 spark-worker-2; do
        docker exec "$worker" bash -c "
            rm -rf /opt/spark/work/* /opt/spark/logs/* /tmp/spark-* 2>/dev/null || true
        " 2>/dev/null || warn "Could not clean $worker"
    done
}

cleanup_system() {
    info "System cleanup"
    docker system prune -f >/dev/null 2>&1 || true
    sync
    echo 3 | sudo tee /proc/sys/vm/drop_caches >/dev/null 2>&1 || true
    
    local new_usage=$(get_disk_usage)
    log "Cleanup complete. Disk usage: ${new_usage}%"
}

perform_full_cleanup() {
    local reason="$1"
    log "🧹 FULL CLEANUP: $reason"
    
    cleanup_airflow_logs
    cleanup_spark_resources
    cleanup_system
}

# =============================================================================
# CONTAINER MANAGEMENT
# =============================================================================

restart_streaming_containers() {
    log "🔄 Restarting streaming containers"
    stop_all_streaming_dags
    sleep 30
    
    docker restart \
        spark-worker-1 \
        spark-worker-2 \
        airflow-spark-airflow-worker-1 \
        >/dev/null 2>&1 || warn "Some containers failed to restart"
    
    sleep 90
    log "✅ Streaming containers restarted"
}

# =============================================================================
# PHASE HANDLERS
# =============================================================================

handle_prep_phase() {
    phase "PRE-BATCH PREPARATION"
    
    for dag in "${STREAMING_DAGS[@]}"; do
        if is_dag_running_cli "$dag"; then
            stop_dag_safely "$dag"
        fi
    done
    log "🛑 Streaming DAGs stopped for batch"
    
    perform_full_cleanup "pre-batch preparation"
    restart_streaming_containers
}

handle_batch_phase() {
    phase "BATCH PROCESSING"
    
    local disk_usage=$(get_disk_usage)
    local batch_status=$(get_batch_status)
    
    info "Batch: $batch_status | Disk: ${disk_usage}%"
    
    if [ $disk_usage -gt $BATCH_DISK_EMERGENCY ]; then
        error "EMERGENCY: Disk ${disk_usage}% > ${BATCH_DISK_EMERGENCY}%"
        stop_all_streaming_dags
        perform_full_cleanup "emergency during batch"
    elif [ $disk_usage -gt $BATCH_DISK_CLEANUP ]; then
        warn "High disk usage: ${disk_usage}%"
        perform_full_cleanup "high usage during batch"
    fi
}

handle_recovery_phase() {
    phase "POST-BATCH RECOVERY"
    
    perform_full_cleanup "post-batch recovery"
    restart_streaming_containers
    
    log "🚀 Resuming streaming"
    start_all_streaming_dags
}

handle_streaming_phase() {
    phase "STREAMING MODE"
    
    local disk_usage=$(get_disk_usage)
    local streaming_status=$(get_streaming_status)
    
    info "Streaming: $streaming_status | Disk: ${disk_usage}%"
    
    if [ $disk_usage -gt $DISK_EMERGENCY_AT ]; then
        error "EMERGENCY: Disk ${disk_usage}% > ${DISK_EMERGENCY_AT}%"
        stop_all_streaming_dags
        perform_full_cleanup "emergency during streaming"
    elif [ $disk_usage -gt $DISK_CLEANUP_AT ]; then
        warn "High disk usage: ${disk_usage}%"
        # Stop producer only, keep consumer running
        for dag in "${STREAMING_DAGS[@]}"; do
            if [[ "$dag" == *"producer"* ]]; then
                stop_dag_safely "$dag"
            fi
        done
        perform_full_cleanup "high usage during streaming"
    else
        # Ensure streaming DAGs are running
        for dag in "${STREAMING_DAGS[@]}"; do
            if ! is_dag_running_cli "$dag"; then
                start_dag_safely "$dag"
                sleep 30
            fi
        done
    fi
}

# =============================================================================
# MAIN MONITOR LOOP
# =============================================================================

main_monitor() {
    log "🚀 REAL-TIME TRENDS MANAGER STARTED"
    log "📅 Batch window: ${BATCH_START_HOUR}:00 - ${BATCH_END_HOUR}:00"
    log "⏰ Current time: $(date '+%H:%M')"
    log "💾 Disk usage: $(get_disk_usage_gb)"
    log "📡 Real-time DAGs: ${STREAMING_DAGS[*]} (hourly restarts)"
    log "🏭 Batch DAGs: ${BATCH_DAGS[*]}"
    log "🎯 Purpose: 30-second rolling dashboard with latest data"
    log "🌐 Airflow UI: http://localhost:8085"
    
    # Validate environment
    if ! get_auth_header >/dev/null 2>&1; then
        exit 1
    fi
    
    # Validate DAGs exist
    for dag in "${STREAMING_DAGS[@]}" "${BATCH_DAGS[@]}"; do
        if ! dag_exists "$dag"; then
            error "DAG $dag not found in Airflow"
            exit 1
        fi
    done
    
    log "✅ Environment validation complete"
    
    while true; do
        local mode=$(get_operation_mode)
        local minutes_to_next=$(get_minutes_until_next_phase)
        local disk_usage=$(get_disk_usage)
        
        log "📊 Mode: $mode | Disk: ${disk_usage}% | Next: ${minutes_to_next}min"
        
        case $mode in
            prep) 
                handle_prep_phase
                sleep $TRANSITION_CHECK ;;
            batch) 
                handle_batch_phase
                sleep $BATCH_CHECK ;;
            recovery) 
                handle_recovery_phase
                sleep $TRANSITION_CHECK ;;
            streaming) 
                handle_streaming_phase
                sleep $STREAMING_CHECK ;;
        esac
    done
}

# =============================================================================
# STATUS AND CONTROL
# =============================================================================

show_status() {
    local mode=$(get_operation_mode)
    local disk_usage=$(get_disk_usage)
    local disk_details=$(get_disk_usage_gb)
    local minutes_to_next=$(get_minutes_until_next_phase)
    local streaming_status=$(get_streaming_status)
    local batch_status=$(get_batch_status)
    
    echo "📊 AIRFLOW 3.0 STREAMING MANAGER STATUS"
    echo "====================================="
    echo "⏰ Time: $(date '+%H:%M:%S')"
    echo "🔄 Mode: $mode"
    echo "⏳ Next phase: ${minutes_to_next} minutes"
    echo "💾 Disk: $disk_details"
    echo "📡 Streaming: $streaming_status"
    echo "🏭 Batch: $batch_status"
    echo "🔒 Safe mode: $SAFE_MODE"
    echo "🌐 UI: http://localhost:8085"
    echo ""
    
    # Test API connectivity
    if command -v docker >/dev/null && [ -n "${AIRFLOW_USER:-}" ]; then
        local auth=$(get_auth_header)
        local api_test=$(docker exec airflow-spark-airflow-apiserver-1 \
            curl -s -o /dev/null -w "%{http_code}" -u "$auth" \
            "http://localhost:8080/api/v2/monitor/health" 2>/dev/null || echo "000")
        if [ "$api_test" = "200" ]; then
            echo "✅ Airflow 3.0 API v2: Connected"
        else
            echo "❌ Airflow 3.0 API v2: Failed (HTTP $api_test)"
        fi
    else
        echo "⚠️  Set AIRFLOW_USER and AIRFLOW_PASS to test API"
    fi
}

show_help() {
    cat << EOF
🎯 AIRFLOW 3.0 BATCH-AWARE STREAMING MANAGER

USAGE: $0 <command>

COMMANDS:
    monitor     - Start continuous monitoring
    status      - Show current system status
    start       - Start all streaming DAGs
    stop        - Stop all streaming DAGs
    cleanup     - Manual cleanup
    help        - Show this help

REQUIREMENTS:
    • Apache Airflow 3.0+
    • Streaming DAGs must have schedule=None
    • Environment variables: AIRFLOW_USER, AIRFLOW_PASS

SETUP:
    export AIRFLOW_USER=admin
    export AIRFLOW_PASS=your_password
    screen -S streaming -dm $0 monitor

FEATURES:
    ✅ Airflow 3.0 API v2 compliant
    ✅ Secure credential handling
    ✅ Intelligent phase management
    ✅ Resource monitoring & cleanup
    ✅ Safe DAG triggering with cooldowns
    ✅ Queue overflow prevention

EOF
}

# =============================================================================
# MAIN
# =============================================================================

case "${1:-help}" in
    monitor) main_monitor ;;
    status) show_status ;;
    start) start_all_streaming_dags ;;
    stop) stop_all_streaming_dags ;;
    cleanup) perform_full_cleanup "manual cleanup" ;;
    help|--help|-h) show_help ;;
    *) echo "❌ Unknown command: $1"; show_help; exit 1 ;;
esac