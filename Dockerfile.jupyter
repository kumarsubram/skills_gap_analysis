FROM spark:4.0.0-python3

USER root

# Your exact Python 3.12 setup (proven working) + Git for jupyterlab-git
RUN apt-get update && apt-get install -y \
    software-properties-common \
    curl \
    wget \
    git \
    && rm -rf /var/lib/apt/lists/*

RUN add-apt-repository ppa:deadsnakes/ppa && \
    apt-get update && \
    apt-get install -y \
    python3.12 \
    python3.12-dev \
    && rm -rf /var/lib/apt/lists/*

# Install pip for Python 3.12
RUN curl -sS https://bootstrap.pypa.io/get-pip.py | python3.12

# Use the correct Java path from the base image and fix Spark scripts
ENV JAVA_HOME=/opt/java/openjdk
ENV PATH="${JAVA_HOME}/bin:${PATH}"

# Fix the spark-class script to use the correct Java path
RUN sed -i 's|/usr/lib/jvm/java-17-openjdk-amd64|/opt/java/openjdk|g' /opt/spark/bin/spark-class || true

# Set Python 3.12 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1 && \
    update-alternatives --install /usr/bin/python python /usr/bin/python3.12 1

# Set up PySpark paths (same as your working setup)
ENV PYTHONPATH="${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip"
ENV PYSPARK_PYTHON=python3.12
ENV PYSPARK_DRIVER_PYTHON=python3.12

# Override any hardcoded Java paths in Spark configuration
ENV SPARK_CONF_DIR=/opt/spark/conf
RUN mkdir -p ${SPARK_CONF_DIR} && \
    echo "JAVA_HOME=/opt/java/openjdk" > ${SPARK_CONF_DIR}/spark-env.sh && \
    echo "export JAVA_HOME=/opt/java/openjdk" >> ${SPARK_CONF_DIR}/spark-env.sh

# ðŸš€ ADD DELTA LAKE JARS (EXACTLY LIKE YOUR WORKING SPARK DOCKERFILE)
RUN echo "ðŸ“¦ Installing Delta Lake JARs..." && \
    wget -q https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/4.0.0/delta-spark_2.13-4.0.0.jar \
         -O /opt/spark/jars/delta-spark_2.13-4.0.0.jar && \
    wget -q https://repo1.maven.org/maven2/io/delta/delta-storage/4.0.0/delta-storage-4.0.0.jar \
         -O /opt/spark/jars/delta-storage-4.0.0.jar && \
    wget -q https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar \
         -O /opt/spark/jars/antlr4-runtime-4.9.3.jar && \
    echo "âœ… Delta Lake JARs installed"

# ADD S3A SUPPORT JARS (EXACTLY LIKE YOUR WORKING SPARK DOCKERFILE)
RUN echo "ðŸ“¦ Installing S3A JARs (Spark 4.0 compatible - proven working)..." && \
    # Remove any existing S3A JARs
    rm -f /opt/spark/jars/hadoop-aws-*.jar /opt/spark/jars/aws-*sdk*.jar && \
    # Use hadoop-aws 3.3.6 (AWS SDK V1) - most stable for Spark 4.0
    wget -q https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.6/hadoop-aws-3.3.6.jar \
         -O /opt/spark/jars/hadoop-aws-3.3.6.jar && \
    wget -q https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.367/aws-java-sdk-bundle-1.12.367.jar \
         -O /opt/spark/jars/aws-java-sdk-bundle-1.12.367.jar && \
    echo "âœ… S3A JARs installed (hadoop-aws 3.3.6 + AWS SDK V1 1.12.367 - proven stable)"

# Install Python packages with LATEST JupyterLab
RUN python3.12 -m pip install --no-cache-dir \
    py4j==0.10.9.9 \
    pandas==2.3.0 \
    pyarrow==20.0.0 \
    jupyter \
    jupyterlab>=4.0 \
    jupyter-collaboration \
    jupyterlab-git \
    jupyterlab-lsp \
    python-lsp-server

# Verify setup
RUN python3.12 --version && \
    python3.12 -c "import py4j; print('py4j available')" && \
    ls -la /opt/spark/jars/delta* && \
    echo "Setup complete"

# Create jovyan user for Jupyter
RUN useradd -m -s /bin/bash jovyan && \
    mkdir -p /home/jovyan/work && \
    chown -R jovyan:spark /home/jovyan

USER jovyan
WORKDIR /home/jovyan/work

# Start Jupyter Lab
CMD ["jupyter", "lab", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root"]