from pyspark.sql import SparkSession

def main():
    # Create Spark session
    spark = SparkSession.builder \
        .appName("SimpleWorkingTest") \
        .getOrCreate()
        
    print("ğŸš€ Spark cluster job started!")
    print(f"ğŸ“Š Spark version: {spark.version}")
    print(f"ğŸ”§ Master URL: {spark.sparkContext.master}")
    print(f"ğŸ­ App ID: {spark.sparkContext.applicationId}")
    print(f"ğŸ Python version being used: {spark.sparkContext.pythonVer}")
    
    # Simple RDD test (lighter than DataFrame operations)
    print("\nğŸ”¢ Testing simple RDD operations...")
    numbers = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    
    # Basic operations
    total = numbers.sum()
    count = numbers.count()
    max_val = numbers.max()
    
    print(f"âœ… Sum: {total}")
    print(f"âœ… Count: {count}")
    print(f"âœ… Max: {max_val}")
    
    # Test distributed processing
    print("\nğŸ”„ Testing distributed map operation...")
    squared = numbers.map(lambda x: x * x).collect()
    print(f"âœ… Squared numbers: {squared}")
    
    print("\nğŸ‰ All tests completed successfully!")
    print("âœ… Distributed Spark processing is working!")
    
    # Stop Spark session
    spark.stop()

if __name__ == "__main__":
    main()