from pyspark.sql import SparkSession

def main():
    # Create Spark session
    spark = SparkSession.builder \
        .appName("CleanSparkTest") \
        .getOrCreate()
    
    print("ğŸš€ Spark job started!")
    print(f"ğŸ“Š Spark version: {spark.version}")
    print(f"ğŸ”§ Master: {spark.sparkContext.master}")
    
    # Simple RDD operations (lightweight)
    print("\nğŸ”¢ Creating data...")
    numbers = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
    
    print("âœ… Computing sum...")
    total = numbers.sum()
    
    print("âœ… Computing count...")
    count = numbers.count()
    
    print(f"âœ… Results: Sum={total}, Count={count}")
    print("ğŸ‰ Job completed successfully!")
    
    spark.stop()

if __name__ == "__main__":
    main()